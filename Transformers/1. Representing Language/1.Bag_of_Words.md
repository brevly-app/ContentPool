
## Bag of Words

---

## The first problem

**How do you turn text into numbers?**

Computers don’t understand words.
They only understand numbers.

So before modeling language, the first question was:

> *How do we represent text numerically?*

---

## The bag-of-words idea

The simplest idea:

> A document is just a collection of words.

So we:

* Build a **vocabulary** : list of all words in corpus
* Represent a text as counts of words appearing in it

Example:
 
 “*I love machine learning*”

Becomes:
> { "I": 1, "love": 1, "machine": 1, "learning": 1 }

---

## Why this felt reasonable

Bag-of-Words made sense because:

* Many tasks care about *which* words appear
* Not *how* they appear

* **Spam detection**  
  {`free`, `win`, `offer`, `click`} → spam

* **Topic classification**  
  {`election`, `policy`, `government`} → politics  
  {`goal`, `match`, `league`} → sports

* **Basic sentiment analysis**  
  {`great`, `excellent`} → positive  
  {`terrible`, `awful`} → negative

For a while, this felt like progress.

---

## The assumption underneath

Bag-of-words assumes:

* Each word is an independent feature

This mirrors tabular data:

* Columns don’t depend on each other
* Column order doesn’t matter

The idea was borrowed from what already worked in machine learning.

---

## What bag-of-words captures well

Bag-of-words is good at:

* Identifying topics
* Detecting keywords
* Measuring word frequency

It answers:

> “What is this text about?”

---

## But it still misses meaning

Consider:

> “This movie tries very hard to be **good**.”

Bag-of-Words sees:

{`good`:1, `tries`:1, `hard`:1}

What the model predicts:
- Positive sentiment (contains `good`)

What the sentence actually means:
- The movie fails to be good

---

## Order is invisible

Compare:

* “Dog bites man”
* “Man bites dog”

Same words.
Same counts.
Different meaning.

Bag-of-words can’t tell **who did what**.

---

## No sense of relationships

Bag-of-words:

* Treats words in isolation
* Ignores dependencies
* Ignores context

The word “bank” looks the same:

* In finance
* Near a river
* As a verb

---

## The obvious next step

Instead of counting individual words:

* Count **pairs** of words
* Or short sequences of words

Examples:

* “not good”
* “very happy”
* “machine learning”

> This approach is called **n-grams**.

It keeps things simple.
But tries to fix what bag-of-words misses.

---