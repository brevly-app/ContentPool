## n-grams

---

## The problem with bag-of-words

Bag-of-words ignores order.

That breaks meaning in obvious cases:

* “not good”
* “dog bites man”

So the next idea followed:

> *What if we keep counts, but with some order?*

---

## The n-gram idea

> Instead of counting **single words**, count **short sequences of words**.

Examples:

* Unigram (1 word) → “good” 
* Bigram (2 words) → “not good”
* Trigram (3 words) → “machine learning model”

These sequences are called **n-grams**.

---

## What n-grams add

n-grams introduce:

* Local word order
* Short-range context
* Simple structure

Now the model can tell the difference between:

* “good”
* “not good”

---

## Why n-grams worked better

n-grams improved performance on:

* Language modeling
* Speech recognition
* Machine translation (early systems)

They captured:

* Common phrases
* Local patterns
* Collocations (“New York”, “credit card”)

> Still simple and count-based.

---

## What the model is really learning

With n-grams, the model learns probabilities like:

> “If I see *not*, how often does *good* come next?”

This is the first time models start answering:

> *What usually comes next in language?*

This question turns out to be extremely important later.

---

## The hidden assumption

n-grams assume:

> **Local context is enough.**

They believe:

* Nearby words matter most
* Distant words matter less (or not at all)

This assumption works sometimes.
And fails often.

---

## Context windows don’t scale

As n grows:

* Vocabulary explodes
* Data sparsity explodes
* Memory usage explodes

Example:

* Vocabulary size = 50k
* Possible bigrams ≈ 50k²
* Possible trigrams ≈ 50k³

Most n-grams are **never seen**.

---

## The sparsity problem

If the model has never seen:

* “astonishingly good movie”

It has no way to reason about it.

n-grams can’t generalize:

* Similar meanings
* Synonyms
* Paraphrases

> n-gram model memorize can not understand.

---

## Still no real understanding

Even with large n:

* Long-range dependencies are missed
* Structure is shallow
* Meaning is still fragmented

Example:

> “The book that you gave me yesterday was fascinating”

The verb “was” depends on “book”,
not the nearby word “yesterday”.

n-grams can’t track this.

---

## A pattern starts to emerge

So far:

* Bag-of-words ignores order
* n-grams add local order
* But both rely on **counting**

This raises a deeper question:

> *Are we missing something by just counting words and phrases?*

---

## Slide 11 — The next shift

To move forward, models needed:

* Generalization beyond seen phrases
* A notion of similarity
* A way to share information across contexts

This is where **distributed representations** enter the picture.

> Words stop being IDs.
> They start becoming **vectors**.

---

## Where we go next

The next step changes everything:

**Word embeddings**

Instead of:

* One dimension per word

We learn:

* Dense vectors
* Where distance means similarity

This is the moment **meaning** starts to emerge.

---