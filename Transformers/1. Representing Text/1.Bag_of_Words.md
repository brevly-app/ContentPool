
## Bag of Words

---

## The first problem

**How do you turn text into numbers?**

Computers don’t understand words.
They only understand numbers.

So before modeling language, the first question was:

> *How do we represent text numerically?*

---

## The bag-of-words idea

The simplest idea:

> A document is just a collection of words.

So we:

**Algorithm (Bag-of-Words)**

1. Collect all documents
2. Build a **vocabulary** (unique words in the corpus)
3. For each document, **count how many times each word appears**
4. Store those counts as a vector

---

## An example

Corpus (2 documents):

1. “I love machine learning”
2. “I love deep learning”

**Step 1 — Vocabulary**

```
[I, love, machine, deep, learning]
```

**Step 2 — Vector representation**

| Document | I | love | machine | deep | learning |
| -------- | - | ---- | ------- | ---- | -------- |
| Doc 1    | 1 | 1    | 1       | 0    | 1        |
| Doc 2    | 1 | 1    | 0       | 1    | 1        |

> Each document is now a **fixed-length numeric vector**.

---

## Why this felt reasonable

Bag-of-Words made sense because:

* Many tasks care about *which* words appear
* Not *how* they appear

* **Spam detection**  
  {`free`, `win`, `offer`, `click`} → spam

* **Topic classification**  
  {`election`, `policy`, `government`} → politics  
  {`goal`, `match`, `league`} → sports

* **Basic sentiment analysis**  
  {`great`, `excellent`} → positive  
  {`terrible`, `awful`} → negative

For a while, this felt like progress.

---

## The assumption underneath

Bag-of-words assumes:

* Each word is an independent feature

This mirrors tabular data:

* Columns don’t depend on each other
* Column order doesn’t matter

The idea was borrowed from what already worked in machine learning.

---

## What bag-of-words captures well

Bag-of-words is good at:

* Identifying topics
* Detecting keywords
* Measuring word frequency

It answers:

> “What is this text about?”

---

## But it still misses meaning

Consider:

> “This movie tries very hard to be **good**.”

Bag-of-Words sees:

{`good`:1, `tries`:1, `hard`:1}

What the model predicts:
- Positive sentiment (contains `good`)

What the sentence actually means:
- The movie fails to be good

---

## Order is invisible

Compare:

* “Dog bites man”
* “Man bites dog”

Same words.
Same counts.
Different meaning.

Bag-of-words can’t tell **who did what**.

---

## No sense of relationships

Bag-of-words:

* Treats words in isolation
* Ignores dependencies
* Ignores context

The word “bank” looks the same:

* In finance
* Near a river
* As a verb

---

## The obvious next step

Instead of counting individual words:

* Count **pairs** of words
* Or short sequences of words

Examples:

* “not good”
* “very happy”
* “machine learning”

> This approach is called **n-grams**.

It keeps things simple.
But tries to fix what bag-of-words misses.

---