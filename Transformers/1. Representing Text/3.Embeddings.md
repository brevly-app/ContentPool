## Word embeddings

---

## The problem with n-grams

n-grams rely on **exact matches**.

Suppose the training data contains:

> “This is a **great movie**”
> “This is an **amazing movie**”

But the model has **never seen**:

> “This is an **astonishing movie**”

---

## The problem with n-grams

An n-gram model treats these as unrelated:

* `great movie` → seen
* `amazing movie` → seen
* `astonishing movie` → unseen → meaningless

Even though to a human:

> astonishing ≈ amazing ≈ great

So the next question became:

> *Can a model learn that different words can carry similar meaning?*

---

## A new idea begins to form

Instead of treating words as:

* Discrete IDs
* Sparse counts
* One-hot vectors

What if we represent each word as:

> **A point in a continuous semantic space**

Where:

* Similar words are close together
* Dissimilar words are far apart

This is the idea of **word embeddings**.

---

## Words as vectors

In this view:

* Each word → a dense vector
* Each vector → a location in space

So instead of:

```
great = [0,0,1,0,0]
amazing = [0,1,0,0,0]
```

We learn something like:

```
great       → [ 0.42, -0.11, 0.89, ... ]
amazing     → [ 0.39, -0.09, 0.91, ... ]
astonishing → [ 0.41, -0.10, 0.88, ... ]
```

These vectors end up **close together**.

Distance now encodes meaning.

---

## Where meaning comes from

The core principle:

> **Words that appear in similar contexts have similar meanings.**

Example contexts:

* “a ___ movie”
* “the ___ performance”
* “absolutely ___”

Words that often fill the same blank:

* great
* amazing
* astonishing

End up with **similar vectors**, because the model must treat them interchangeably to predict context well.

This idea is called the **distributional hypothesis**.

---

## How embeddings are learned

Take a sentence:

> “This movie was amazing”

During training, the model might learn tasks like:

* Given `amazing`, predict nearby words:
  → `movie`, `was`
* Given `movie was`, predict the next word:
  → `amazing`

To succeed, the model must assign vectors so that:

* `amazing`, `great`, `fantastic` behave similarly
* Substituting one doesn’t break predictions

Meaning **emerges from prediction**, not labels.

---

## What embeddings immediately fix

Compared to n-grams, embeddings:

* Generalize across similar words
* Reduce sparsity
* Share statistical strength

Now the model knows:

* “amazing movie” ≈ “great movie”
* Even if one was never seen before

This is a huge step forward.

---

## Famous early results

Embeddings revealed something surprising:

> Vector arithmetic worked on words

Examples (approximately):

* king − man + woman ≈ queen
* Paris − France + Italy ≈ Rome

No one programmed this.

It fell out of the geometry.

---

## Why this felt like a breakthrough

For the first time:

* Models weren’t just counting
* They were **organizing meaning**

Language started to look:

* Continuous, not discrete
* Structured, not sparse

This shifted NLP firmly into the neural era.

---

## But something was still missing

A word embedding assigns:

* **One vector per word**

That means:

* “bank” has one meaning
* “light” has one meaning
* “charge” has one meaning

Context is ignored *again*.

---

## The same word, different meanings

Consider:

* “I sat by the river *bank*”
* “I went to the *bank* to withdraw money”

Same word.
Same vector.

The model has no way to tell which meaning is intended.

---

## The limitation becomes clear

Word embeddings capture:

* Semantic similarity across words

But they do **not** capture:

* Context-dependent meaning
* Sentence-level interpretation
* Long-range dependencies

Meaning is still mostly **static**.

---

## The same word, different meanings

Consider:

* “I sat by the river *bank*”
* “I went to the *bank* to withdraw money”

Same word.
Same vector.

The representation doesn’t change —
even though the meaning clearly does.

---

## A shift in thinking

This led to a deeper realization:

> *Meaning is not a property of words.*
> *Meaning is a property of sequences.*

So instead of asking:

* “What does this word mean?”

The new question became:

* “What does this word mean **given the context**?”

---

## Shifting focus: from words to sequences

Individual word vectors are useful, but language is about:

* Sequences of words
* How likely a sentence is
* What word should come next

This reframes the goal entirely.

Instead of asking:

> *How do we represent words?*

We now ask:

> *How do we model language itself?*

---

## Language as a prediction problem

If a model truly understands language, it should be able to answer:

> “Given what I’ve seen so far, what likely comes next?”

This idea connects:

* Fluency
* Grammar
* Meaning
* Context

All into a single objective.

---

## Next section

We’ll start by defining:

* What a language model is
* What it learns
* Why almost all modern NLP systems are built on this idea
