## What is language modeling?

---

## From representation to modeling

So far, we’ve talked about **how to represent text**:

* Bag of words
* n-grams
* Word vectors

But representation alone is not modeling.

The next question is deeper:

> *What does it mean to model a language?*

---

## Language as a probabilistic process

At its core, language modeling asks:

> **How likely is a sequence of words?**

Or equivalently:

> *Given some words, what comes next?*

This turns language into a **probability problem**.

---

## A simple example

Consider this prefix:

> “I want to drink a cup of ____ ”

Humans instantly expect:

* “coffee”
* “tea”
* “water”

Not:

* “democracy”
* “television”

> A language model tries to assign **high probability** to likely continuations
> and **low probability** to unlikely ones.

---

## The formal goal (without heavy math)

We start with language as a **sequence** of words:

$$w₁, w₂, w₃, …, wₙ$$

A **language model’s** job is to estimate:

$$P(w₁, w₂, …, wₙ)$$

In simple words:

> *How likely is this entire sentence to exist in the language?*

---

## Why we don’t model this directly

Modeling the full joint probability is hard because:

* The number of possible sequences is enormous
* Words strongly depend on earlier words
* We need a tractable way to learn dependencies

> So we use a basic probability identity: Chain rule

---

## The key breakdown (chain rule)

Any joint probability can be rewritten as:

$$
P(w_1,\dots,w_n) = \prod_{i=1}^{n} P(w_i \mid w_1,\dots,w_{i-1})
$$

> Joint probability is the product of conditional probabilities
> of the next word given each prefix

---

## What the model actually learns

Instead of learning a joint probability, the model learns:

> **At each position:**
> *“Given everything so far, what comes next?”*

So training reduces to learning:

> **P(wₙ | w₁, w₂, …, wₙ₋₁)**

This is called **next-token prediction**.

---

## Why this reframing is powerful

Next-token prediction:

* Turns language modeling into a **supervised learning problem**
* Uses *every* position in *every* sentence as a training signal
* Naturally encodes **context, order, and dependency**

> One sentence of length *n* gives **n−1 training examples**.

---

## What this implies (important intuition)

If a model is good at next-token prediction, it must:

* Understand syntax (what’s grammatically valid)
* Understand semantics (what makes sense)
* Track long-range dependencies
* Use context dynamically

There is **no shortcut**.
predicting the next word *requires* understanding the sentence so far.

---

## What a language model actually learns

A good language model captures:

* Which words tend to follow others
* Which sequences sound natural
* Which constructions are grammatical

It doesn’t “know” rules explicitly.
it **learns them from data**.

---

## Generation comes for free

Once you can predict the next word:

* You can generate text
* One word at a time
* Feeding predictions back as input

This is why:

* Text generation
* Translation
* Chatbots

All start with language modeling.

---

## Scoring language

Language models can also **score** text.

Given two sentences:

* “She enjoys reading books”
* “She enjoy reading books”

A language model assigns higher probability to the first.

This makes language models useful even without generation.

---

## Where we go next

Now that we know what language modeling is, the next step is obvious:

> *How do we estimate these probabilities in practice?*

The first serious answer was:
**n-gram language models**.