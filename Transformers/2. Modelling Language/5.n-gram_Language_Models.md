# n-gram Language Models

---

## From language modeling to estimation

We want to model language by learning:

> **P(next word | previous words)**

The most direct way to do this is:

* Look at text
* Count how often a sequence occur
* Convert those counts into probabilities

This leads naturally to **n-gram language models**.

---

## The basic idea

An **n-gram** is a sequence of *n* consecutive words.

Examples:

* Unigram (n = 1): `movie`
* Bigram (n = 2): `good movie`
* Trigram (n = 3): `very good movie`

An n-gram language model assumes:

> **The next word depends only on the previous (n−1) words.**

This is the **Markov assumption**.

---

## What the model is actually estimating

Given a sequence $w₁, w₂, …, wₙ$

The model estimates:

$$P(wₙ | wₙ₋₁, …, wₙ₋₍ₙ₋₁₎)$$

Using only:

* Exact counts from data
* Relative frequencies

---

## Example corpus

Suppose the training data contains the following sentences:

* “I want to drink coffee”
* “I want to drink tea”
* “I want to drink water”
* “I want to eat food”

We build a **trigram language model**
(context size = 3 words).

---

## Turning text into a probability table

First, we count occurrences of:

> Prefix (last 3 words) → next word

* `I want to drink` → coffee (1)
* `I want to drink` → tea (1)
* `I want to drink` → water (1)
* `I want to eat` → food (1)

Then we normalize counts into probabilities.

---

## The actual n-gram model (lookup table)

**Trigram language model**

| Prefix (context)  | coffee | tea | water | food |
| ----------------- | ------ | --- | ----- | ---- |
| `I want to drink` | 1/3    | 1/3 | 1/3   | 0    |
| `I want to eat`   | 0      | 0   | 0     | 1    |

Each cell stores:

> **P(next token | prefix)**

This table *is* the model.

---

## How prediction works

To predict the next word after:

> “I want to drink ___”

The model:

1. Looks up the row `I want to drink`
2. Reads the probability distribution
3. Samples or selects the most likely word

No reasoning.
No understanding.
Just table lookup.

---

## What information the model uses

An n-gram language model uses:

* Only the last (n−1) words
* Exact word matches
* Observed frequencies

Everything outside the window is ignored.
Everything unseen has probability zero (or near zero, with smoothing).

---

## Context is fixed and local

The size of *n* fixes the context:

* Small n → short memory
* Large n → massive tables

There is no mechanism to:

* Expand context when needed
* Shrink context when irrelevant

The window is rigid.

---

## The sparsity problem (made explicit)

If the corpus never contains:

> “I want to drink juice”

Then the row:

* `I want to drink` → juice

Has **no entry**.

The model cannot infer anything from:

* “I want to drink water”
* “I want to drink tea”

Because those probabilities live in *different columns*.

---

## No generalization across phrases

The table treats these as unrelated:

* `amazing movie`
* `great movie`
* `astonishing movie`

Even if their meanings are similar, their rows are separate.

Counts do not share information.

---

## Long-range dependencies are invisible

Consider:

> “The book that you gave me yesterday was fascinating”

Choosing **fascinating”** requires remembering **“book”**.

An n-gram model only sees the most recent few words.

Anything earlier is discarded before lookup.

---

## The question this leaves open

n-gram models show that:

* Next-word prediction is meaningful
* Language can be modeled probabilistically

But they raise a deeper question:

> *Can we estimate these probabilities without storing a separate row for every possible prefix?*

---

## Where we go next

**Embedding-based language models**

Same objective:

* Predict the next word

Different approach:

* Learn continuous representations
* Share statistical strength across similar contexts

---
